{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#111111111222222222233333333334444444444555555555566666666667777777777888888888899999999990000000000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a spark session\n",
    "import pyspark\n",
    "spark_context = pyspark.SparkContext()\n",
    "spark_session = pyspark.sql.SparkSession(spark_context)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+----------------+-------------------+----------------+----------------+------------------+-----------------+-----------------+------------------+-----------------+------------------+------------------+------------------+------------------+------------------+----------------+------------------+-----------------+------------------+-----------------+-----------------+------------------+-----------------+------------------+------------------+-----------------+------------------+-----------------+-------------------+------+-----+\n",
      "|Time|              V1|                 V2|              V3|              V4|                V5|               V6|               V7|                V8|               V9|               V10|               V11|               V12|               V13|               V14|             V15|               V16|              V17|               V18|              V19|              V20|               V21|              V22|               V23|               V24|              V25|               V26|              V27|                V28|Amount|Class|\n",
      "+----+----------------+-------------------+----------------+----------------+------------------+-----------------+-----------------+------------------+-----------------+------------------+------------------+------------------+------------------+------------------+----------------+------------------+-----------------+------------------+-----------------+-----------------+------------------+-----------------+------------------+------------------+-----------------+------------------+-----------------+-------------------+------+-----+\n",
      "|   0|-1.3598071336738|-0.0727811733098497|2.53634673796914|1.37815522427443|-0.338320769942518|0.462387777762292|0.239598554061257|0.0986979012610507|0.363786969611213|0.0907941719789316|-0.551599533260813|-0.617800855762348|-0.991389847235408|-0.311169353699879|1.46817697209427|-0.470400525259478|0.207971241929242|0.0257905801985591|0.403992960255733|0.251412098239705|-0.018306777944153|0.277837575558899|-0.110473910188767|0.0669280749146731|0.128539358273528|-0.189114843888824|0.133558376740387|-0.0210530534538215|149.62|    0|\n",
      "+----+----------------+-------------------+----------------+----------------+------------------+-----------------+-----------------+------------------+-----------------+------------------+------------------+------------------+------------------+------------------+----------------+------------------+-----------------+------------------+-----------------+-----------------+------------------+-----------------+------------------+------------------+-----------------+------------------+-----------------+-------------------+------+-----+\n",
      "only showing top 1 row\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# load csv file into dataframe\n",
    "df = spark_session.read.csv(\"creditcard.csv\", header=True,\\\n",
    "                    sep=\",\", inferSchema=True)\n",
    "df.show(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- Time: decimal(10,0) (nullable = true)\n",
      " |-- V1: double (nullable = true)\n",
      " |-- V2: double (nullable = true)\n",
      " |-- V3: double (nullable = true)\n",
      " |-- V4: double (nullable = true)\n",
      " |-- V5: double (nullable = true)\n",
      " |-- V6: double (nullable = true)\n",
      " |-- V7: double (nullable = true)\n",
      " |-- V8: double (nullable = true)\n",
      " |-- V9: double (nullable = true)\n",
      " |-- V10: double (nullable = true)\n",
      " |-- V11: double (nullable = true)\n",
      " |-- V12: double (nullable = true)\n",
      " |-- V13: double (nullable = true)\n",
      " |-- V14: double (nullable = true)\n",
      " |-- V15: double (nullable = true)\n",
      " |-- V16: double (nullable = true)\n",
      " |-- V17: double (nullable = true)\n",
      " |-- V18: double (nullable = true)\n",
      " |-- V19: double (nullable = true)\n",
      " |-- V20: double (nullable = true)\n",
      " |-- V21: double (nullable = true)\n",
      " |-- V22: double (nullable = true)\n",
      " |-- V23: double (nullable = true)\n",
      " |-- V24: double (nullable = true)\n",
      " |-- V25: double (nullable = true)\n",
      " |-- V26: double (nullable = true)\n",
      " |-- V27: double (nullable = true)\n",
      " |-- V28: double (nullable = true)\n",
      " |-- Amount: double (nullable = true)\n",
      " |-- Class: integer (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Print the schema to get the column names\n",
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+-----+\n",
      "|            features|class|\n",
      "+--------------------+-----+\n",
      "|[-1.3598071336738...|    0|\n",
      "+--------------------+-----+\n",
      "only showing top 1 row\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.feature import VectorAssembler\n",
    "vectorAssembler = VectorAssembler( inputCols= [ \"V1\",\"V2\",\"V3\",\"V4\",\"V5\",\n",
    "                                                \"V6\",\"V7\",\"V8\",\"V9\",\"V10\",\n",
    "                                                \"V11\",\"V12\",\"V13\",\"V14\",\"V15\",\n",
    "                                                \"V16\",\"V17\",\"V18\",\"V19\",\"V20\"],\n",
    "                                    outputCol= \"features\")\n",
    "vdf = vectorAssembler.transform( df )\n",
    "vdf = vdf.select([\"features\",\"class\"])\n",
    "vdf.show(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+-----+\n",
      "|            features|class|\n",
      "+--------------------+-----+\n",
      "|[-30.552380043581...|    1|\n",
      "+--------------------+-----+\n",
      "only showing top 1 row\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Split data into training and test dataframe\n",
    "df_train, df_test = vdf.randomSplit([0.8, 0.2],seed=5)\n",
    "df_train.show(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a GBT model and train it...\n",
    "from pyspark.ml.regression import GBTRegressor\n",
    "from pyspark.ml import Pipeline\n",
    "gbt = GBTRegressor(featuresCol=\"features\", labelCol=\"class\", maxIter=1)\n",
    "pipeline = Pipeline(stages=[gbt])\n",
    "model = pipeline.fit(df_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+-----+\n",
      "|          prediction|class|\n",
      "+--------------------+-----+\n",
      "|  0.7739130434782608|    0|\n",
      "|  0.7739130434782608|    1|\n",
      "|  0.7739130434782608|    1|\n",
      "|0.031578947368421054|    0|\n",
      "|  0.7739130434782608|    1|\n",
      "|  0.7739130434782608|    1|\n",
      "|0.031578947368421054|    0|\n",
      "|  0.7739130434782608|    1|\n",
      "|  0.7739130434782608|    0|\n",
      "|  0.7739130434782608|    1|\n",
      "|0.002285915245296...|    0|\n",
      "|8.137358613394092E-5|    0|\n",
      "|  0.7739130434782608|    0|\n",
      "|8.137358613394092E-5|    0|\n",
      "|8.137358613394092E-5|    0|\n",
      "|1.919017463058913...|    0|\n",
      "|  0.9476190476190476|    1|\n",
      "|1.919017463058913...|    0|\n",
      "|8.137358613394092E-5|    0|\n",
      "|8.137358613394092E-5|    0|\n",
      "+--------------------+-----+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Make predictions.\n",
    "predictions = model.transform(df_test)\n",
    "predictions = predictions.select(\"prediction\", \"class\")\n",
    "predictions.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+-----+---------+\n",
      "|          prediction|class|predclass|\n",
      "+--------------------+-----+---------+\n",
      "|  0.7739130434782608|    0|        1|\n",
      "|  0.7739130434782608|    1|        1|\n",
      "|  0.7739130434782608|    1|        1|\n",
      "|0.031578947368421054|    0|        0|\n",
      "|  0.7739130434782608|    1|        1|\n",
      "|  0.7739130434782608|    1|        1|\n",
      "|0.031578947368421054|    0|        0|\n",
      "|  0.7739130434782608|    1|        1|\n",
      "|  0.7739130434782608|    0|        1|\n",
      "|  0.7739130434782608|    1|        1|\n",
      "|0.002285915245296...|    0|        0|\n",
      "|8.137358613394092E-5|    0|        0|\n",
      "|  0.7739130434782608|    0|        1|\n",
      "|8.137358613394092E-5|    0|        0|\n",
      "|8.137358613394092E-5|    0|        0|\n",
      "|1.919017463058913...|    0|        0|\n",
      "|  0.9476190476190476|    1|        1|\n",
      "|1.919017463058913...|    0|        0|\n",
      "|8.137358613394092E-5|    0|        0|\n",
      "|8.137358613394092E-5|    0|        0|\n",
      "+--------------------+-----+---------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import udf,col\n",
    "from pyspark.sql.types import IntegerType\n",
    "func = udf(lambda prediction : 1 if prediction>0.5 else 0, IntegerType())\n",
    "npredictions = predictions.withColumn('predclass', func( col(\"prediction\") ) )\n",
    "npredictions.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Root Mean Squared Error (RMSE) on test data = 0.0276316\n"
     ]
    }
   ],
   "source": [
    "# Select (prediction, true label) and compute test error\n",
    "from pyspark.ml.evaluation import RegressionEvaluator\n",
    "evaluator = RegressionEvaluator(\n",
    "    labelCol=\"class\", predictionCol=\"prediction\", metricName=\"rmse\")\n",
    "rmse = evaluator.evaluate(predictions)\n",
    "print(\"Root Mean Squared Error (RMSE) on test data = %g\" % rmse)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AUC Score= 0.9591327570454707\n"
     ]
    }
   ],
   "source": [
    "# Compute an AUC score\n",
    "score = [ row.prediction for row in predictions.select('prediction').collect() ]\n",
    "ground_truth = [ int(row[\"class\"] ) for row in predictions.select('class').collect() ]\n",
    "from sklearn import metrics\n",
    "fpr, tpr, thresholds = metrics.roc_curve(ground_truth, score, pos_label=1)\n",
    "print(\"AUC Score=\", metrics.auc(fpr, tpr))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of not fraud =  56916 , number correctly classified 56894 , accuracy =  0.9996134654578678\n"
     ]
    }
   ],
   "source": [
    "num_not_fraud = npredictions.filter( \\\n",
    "    ( npredictions[\"class\"] == 0 ) ).count()\n",
    "num_not_fraud_correct = npredictions.filter( \\\n",
    "    ( npredictions[\"class\"] == 0 ) & ( npredictions[\"predclass\"] == 0 ) ) \\\n",
    "    .count()\n",
    "print( \"number of not fraud = \", num_not_fraud, \", number correctly classified\", num_not_fraud_correct, \\\n",
    "      \", accuracy = \", num_not_fraud_correct/num_not_fraud )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of fraud =  102 , number correctly classified 74 , accuracy =  0.7254901960784313\n"
     ]
    }
   ],
   "source": [
    "num_fraud = npredictions.filter( \\\n",
    "    ( npredictions[\"class\"] == 1 ) ).count()\n",
    "num_fraud_correct = npredictions.filter( \\\n",
    "    ( npredictions[\"class\"] == 1 ) & ( npredictions[\"predclass\"] == 1 ) ) \\\n",
    "    .count()\n",
    "print( \"number of fraud = \", num_fraud, \", number correctly classified\", num_fraud_correct, \\\n",
    "      \", accuracy = \", num_fraud_correct/num_fraud )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a GBT model and train it...\n",
    "from pyspark.ml.regression import GBTRegressor\n",
    "from pyspark.ml import Pipeline\n",
    "gbt = GBTRegressor(featuresCol=\"features\", labelCol=\"class\", maxIter=10)\n",
    "pipeline = Pipeline(stages=[gbt])\n",
    "model = pipeline.fit(df_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make predictions.\n",
    "predictions = model.transform(df_test)\n",
    "predictions = predictions.select(\"prediction\", \"class\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Root Mean Squared Error (RMSE) on test data = 0.02563\n"
     ]
    }
   ],
   "source": [
    "# Select (prediction, true label) and compute test error\n",
    "from pyspark.ml.evaluation import RegressionEvaluator\n",
    "evaluator = RegressionEvaluator(\n",
    "    labelCol=\"class\", predictionCol=\"prediction\", metricName=\"rmse\")\n",
    "rmse = evaluator.evaluate(predictions)\n",
    "print(\"Root Mean Squared Error (RMSE) on test data = %g\" % rmse)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AUC Score= 0.9544809929734772\n"
     ]
    }
   ],
   "source": [
    "# Compute an AUC score\n",
    "score = [ row.prediction for row in predictions.select('prediction').collect() ]\n",
    "ground_truth = [ int(row[\"class\"] ) for row in predictions.select('class').collect() ]\n",
    "from sklearn import metrics\n",
    "fpr, tpr, thresholds = metrics.roc_curve(ground_truth, score, pos_label=1)\n",
    "print(\"AUC Score=\", metrics.auc(fpr, tpr))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Root Mean Squared Error (RMSE) on test data = 0.0223985\n",
      "AUC Score= 0.9639666816122471\n"
     ]
    }
   ],
   "source": [
    "# Split data into training and test dataframe\n",
    "df_train, df_test = vdf.randomSplit([0.8, 0.2],seed=10)\n",
    "\n",
    "# Create a GBT model and train it...\n",
    "from pyspark.ml.regression import GBTRegressor\n",
    "from pyspark.ml import Pipeline\n",
    "gbt = GBTRegressor(featuresCol=\"features\", labelCol=\"class\", maxIter=10)\n",
    "pipeline = Pipeline(stages=[gbt])\n",
    "model = pipeline.fit(df_train)\n",
    "\n",
    "# Make predictions.\n",
    "predictions = model.transform(df_test)\n",
    "predictions = predictions.select(\"prediction\", \"class\")\n",
    "\n",
    "# Select (prediction, true label) and compute test error\n",
    "from pyspark.ml.evaluation import RegressionEvaluator\n",
    "evaluator = RegressionEvaluator(\n",
    "    labelCol=\"class\", predictionCol=\"prediction\", metricName=\"rmse\")\n",
    "rmse = evaluator.evaluate(predictions)\n",
    "print(\"Root Mean Squared Error (RMSE) on test data = %g\" % rmse)\n",
    "\n",
    "# Compute an AUC score\n",
    "score = [ row.prediction for row in predictions.select('prediction').collect() ]\n",
    "ground_truth = [ int(row[\"class\"] ) for row in predictions.select('class').collect() ]\n",
    "from sklearn import metrics\n",
    "fpr, tpr, thresholds = metrics.roc_curve(ground_truth, score, pos_label=1)\n",
    "print(\"AUC Score=\", metrics.auc(fpr, tpr))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
